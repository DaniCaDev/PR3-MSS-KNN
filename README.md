# ğŸŒ¿ Sistema de ClasificaciÃ³n KNN Extendido

<div align="center">

![Java](https://img.shields.io/badge/Java-ED8B00?style=for-the-badge&logo=openjdk&logoColor=white)
![IntelliJ IDEA](https://img.shields.io/badge/IntelliJ_IDEA-000000?style=for-the-badge&logo=intellij-idea&logoColor=white)
![Licencia MIT](https://img.shields.io/badge/Licencia-MIT-green.svg?style=for-the-badge)

</div>

<p align="center">
  <img src="https://miro.medium.com/v2/resize:fit:1400/0*jqxx3-dJqFjXD6FA" alt="KNN Visualization" width="600"/>
</p>

> Un sistema de clasificaciÃ³n avanzado basado en el algoritmo de k-vecinos mÃ¡s cercanos (k-NN) con mÃºltiples caracterÃ­sticas extendidas, desarrollado como parte de la asignatura de Modelado de Sistemas Software.

## ğŸ“– Ãndice

- [CaracterÃ­sticas](#-caracterÃ­sticas)
- [Estructura del Proyecto](#-estructura-del-proyecto)
- [Diagrama de clases](#-diagrama-de-clases)
- [Requisitos](#-requisitos)
- [InstalaciÃ³n y EjecuciÃ³n](#-instalaciÃ³n-y-ejecuciÃ³n)
- [GuÃ­a de Uso](#-guÃ­a-de-uso)
- [Ejemplos de ClasificaciÃ³n](#-ejemplos-de-clasificaciÃ³n)
- [Fundamentos TeÃ³ricos](#-fundamentos-teÃ³ricos)
- [ImplementaciÃ³n y Arquitectura](#-implementaciÃ³n-y-arquitectura)
- [Licencia](#-licencia)

## âœ¨ CaracterÃ­sticas

El sistema ofrece un conjunto completo de funcionalidades para clasificaciÃ³n mediante k-NN:

- **Lectura y procesamiento de datasets**: Soporte para archivos CSV con mÃºltiples configuraciones
- **Preprocesamiento de datos**:
    - ğŸ”„ NormalizaciÃ³n en rango [0,1]
    - ğŸ“Š EstandarizaciÃ³n (z-score)
    - ğŸ“ˆ Datos crudos (sin preprocesado)
- **EstadÃ­sticas detalladas**:
    - ğŸ“‰ Valores mÃ­nimos, mÃ¡ximos, medias y desviaciones tÃ­picas para atributos numÃ©ricos
    - ğŸ“Š Frecuencias y valores distintos para atributos categÃ³ricos
- **ConfiguraciÃ³n flexible del algoritmo k-NN**:
    - ğŸ›ï¸ Valor configurable de k (nÃºmero de vecinos)
    - ğŸ“ MÃºltiples mÃ©tricas de distancia (EuclÃ­dea, Manhattan, Chebyshev)
    - âš–ï¸ Diferentes estrategias de peso (igualdad, cercanÃ­a, orden fijo)
    - ğŸ¯ Reglas de clasificaciÃ³n personalizables (mayorÃ­a simple, umbral)
- **ExperimentaciÃ³n avanzada**:
    - ğŸ§ª DivisiÃ³n automÃ¡tica de conjuntos de entrenamiento y prueba
    - ğŸ“‹ Matriz de confusiÃ³n para evaluaciÃ³n de rendimiento
    - ğŸ“Š CÃ¡lculo de precisiÃ³n predictiva
- **Sistema de atributos flexible**:
    - ğŸ·ï¸ Soporte para mÃºltiples atributos de clase en posiciones arbitrarias
    - âš–ï¸ Pesado de atributos segÃºn su importancia para la clasificaciÃ³n

## ğŸ§± Estructura del Proyecto

```
src/
â”œâ”€â”€ main/
â”‚   â”œâ”€â”€ java/
â”‚   â”‚   â””â”€â”€ knn/
â”‚   â”‚       â”œâ”€â”€ KNNSystemMain.java                   # Clase principal y menÃº interactivo
â”‚   â”‚       â”œâ”€â”€ model/
â”‚   â”‚       â”‚   â”œâ”€â”€ dataset/
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ Dataset.java                 # Manejo de conjuntos de datos
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ Instance.java                # RepresentaciÃ³n de instancias
â”‚   â”‚       â”‚   â”‚   â”œâ”€â”€ PreprocessingType.java       # Tipos de preprocesamiento
â”‚   â”‚       â”‚   â”‚   â””â”€â”€ attributes/                  # Clases para atributos
â”‚   â”‚       â”‚   â””â”€â”€ classification/
â”‚   â”‚       â”‚       â”œâ”€â”€ KNNClassifier.java           # ImplementaciÃ³n del clasificador
â”‚   â”‚       â”‚       â”œâ”€â”€ Neighbor.java                # Clase auxiliar para vecinos
â”‚   â”‚       â”‚       â”œâ”€â”€ metrics/                     # MÃ©tricas de distancia
â”‚   â”‚       â”‚       â”œâ”€â”€ weights/                     # Estrategias de peso
â”‚   â”‚       â”‚       â””â”€â”€ rules/                       # Reglas de clasificaciÃ³n
â”‚   â”‚       â””â”€â”€ experiment/
â”‚   â”‚           â”œâ”€â”€ Experiment.java                  # GestiÃ³n de experimentos
â”‚   â”‚           â””â”€â”€ ExperimentResult.java            # EvaluaciÃ³n de resultados
â”‚   â””â”€â”€ resources/
â”‚       â”œâ”€â”€ iris.csv                                 # Dataset Iris
â”‚       â””â”€â”€ glass.csv                                # Dataset Glass
â””â”€â”€ test/
    â””â”€â”€ java/
        â””â”€â”€ knn/                                     # Pruebas unitarias
```

## ğŸ“‹ Diagrama de clases

```mermaid
classDiagram
    %% Interfaces y clases abstractas principales
    class Attribute {
        <<abstract>>
        -name: String
        -index: int
        +getName() String
        +getIndex() int
        +isClass() boolean
        +getStatistics() String
    }
    
    class Dataset {
        -instances: List~Instance~
        -attributes: List~Attribute~
        -classAttributeIndices: int[]
        -preprocessingType
        -originalInstances: List~Instance~
        +loadFromFile(filename, hasHeader, delimiter, classIndices) void
        +saveToFile(filename, normalized) void
        +normalize(type) void
        +setAttributeWeights(weights) void
        +split(testPercentage, randomize, seed) Dataset[]
        +getDatasetInfo() String
        +getAttributesInfo() String
        +isClassAttribute(index) boolean
    }
    
    class DistanceMetric {
        <<interface>>
        +calculate(a, b, classIndices, weights) double
        +getName() String
    }
    
    class CaseWeightStrategy {
        <<interface>>
        +calculateWeights(neighbors, k) Map
        +getName() String
    }
    
    class ClassificationRule {
        <<interface>>
        +decide(votes) String
        +getName() String
    }
    
    %% ImplementaciÃ³n de atributos
    class NumericAttribute {
        -min: double
        -max: double
        -mean: double
        -stdDev: double
        -weight: double
        +updateValue(value) void
        +normalize(value, type) double
        +getWeight() double
        +setWeight(weight) void
    }
    
    class CategoricalAttribute {
        -uniqueValues: Set
        -mapping: Map
        -frequencies: Map
        -isClassAttribute: boolean
        +addValue(value) int
        +getValue(code) String
        +getCode(value) int
        +updateFrequencies(instances) void
    }
    
    %% Clase para instancias
    class Instance {
        -values: double[]
        +getValue(index) double
        +setValue(index, value) void
        +getValues() double[]
    }
    
    %% Implementaciones de mÃ©tricas
    class EuclideanDistance {
        +calculate(a, b, classIndices, weights) double
        +getName() String
    }
    
    class ManhattanDistance {
        +calculate(a, b, classIndices, weights) double
        +getName() String
    }
    
    class ChebyshevDistance {
        +calculate(a, b, classIndices, weights) double
        +getName() String
    }
    
    %% Implementaciones de estrategias de peso
    class EqualityWeight {
        +calculateWeights(neighbors, k) Map
        +getName() String
    }
    
    class DistanceBasedWeight {
        +calculateWeights(neighbors, k) Map
        +getName() String
    }
    
    class FixedOrderWeight {
        -weights: double[]
        +calculateWeights(neighbors, k) Map
        +getName() String
    }
    
    %% Implementaciones de reglas
    class MajorityRule {
        +decide(votes) String
        +getName() String
    }
    
    class ThresholdRule {
        -threshold: double
        +decide(votes) String
        +getName() String
    }
    
    %% Clases para clasificaciÃ³n y experimentaciÃ³n
    class KNNClassifier {
        -trainingSet: Dataset
        -k: int
        -metric: DistanceMetric
        -weightStrategy: CaseWeightStrategy
        -classificationRule: ClassificationRule
        -mainClassIndex: int
        +setMetric(metric) void
        +setWeightStrategy(strategy) void
        +setClassificationRule(rule) void
        +classify(instance) String
        +getConfiguration() String
    }
    
    class Neighbor {
        -instance: Instance
        -distance: double
        -index: int
        +getInstance() Instance
        +getDistance() double
        +getIndex() int
    }
    
    class Experiment {
        -trainingSet: Dataset
        -testSet: Dataset
        -classifier: KNNClassifier
        +run() ExperimentResult
        +saveTrainingSet(filename) void
        +saveTestSet(filename) void
    }
    
    class ExperimentResult {
        -accuracy: double
        -confusionMatrix: int[][]
        +getAccuracy() double
        +getResultsInfo() String
    }
    
    class KNNSystemMain {
        +mainMenu() void
        +loadDatasetMenu() void
        +preprocessDataset() void
        +configureClassifier() void
        +experimentationMenu() void
        +classifyNewInstance() void
    }
    
    %% Enumeraciones
    class PreprocessingType {
        <<enumeration>>
        NONE
        RANGE_0_1
        STANDARDIZE
    }
    
    %% Relaciones principales
    Attribute <|-- NumericAttribute
    Attribute <|-- CategoricalAttribute
    
    DistanceMetric <|.. EuclideanDistance
    DistanceMetric <|.. ManhattanDistance
    DistanceMetric <|.. ChebyshevDistance
    
    CaseWeightStrategy <|.. EqualityWeight
    CaseWeightStrategy <|.. DistanceBasedWeight
    CaseWeightStrategy <|.. FixedOrderWeight
    
    ClassificationRule <|.. MajorityRule
    ClassificationRule <|.. ThresholdRule
    
    Dataset o-- Instance
    Dataset o-- Attribute
    
    KNNClassifier --> Dataset
    KNNClassifier --> DistanceMetric
    KNNClassifier --> CaseWeightStrategy
    KNNClassifier --> ClassificationRule
    KNNClassifier --> Neighbor
    
    Experiment --> Dataset
    Experiment --> KNNClassifier
    Experiment --> ExperimentResult
    
    KNNSystemMain --> Dataset
    KNNSystemMain --> KNNClassifier
    KNNSystemMain --> Experiment
```

## ğŸ› ï¸ Requisitos

- Java JDK 8 o superior
- IDE compatible (IntelliJ IDEA recomendado)
- Maven (opcional, para gestiÃ³n de dependencias)

## ğŸ”§ InstalaciÃ³n y EjecuciÃ³n

1. **Clonar el repositorio**:
```bash
git clone https://github.com/tu-usuario/PR3-MSS-KNN.git
cd PR3-MSS-KNN
```

2. **Abrir el proyecto en IntelliJ IDEA**:
    - Selecciona `File > Open` y navega hasta la carpeta del proyecto
    - AsegÃºrate de que la carpeta `src/main/resources` estÃ© marcada como "Resources Root"

3. **Compilar el proyecto**:
    - En IntelliJ: `Build > Build Project`
    - Con Maven: `mvn clean compile`

4. **Ejecutar la aplicaciÃ³n**:
    - En IntelliJ: Ejecuta la clase `knn.KNNSystemMain`
    - Con Maven: `mvn exec:java -Dexec.mainClass="knn.KNNSystemMain"`

## ğŸ“š GuÃ­a de Uso

### 1. Cargar un Dataset

El sistema puede cargar datasets en formato CSV. Puedes usar los datasets de ejemplo incluidos:
- `iris.csv`: 150 instancias de flores Iris con 4 atributos numÃ©ricos y 3 clases
- `glass.csv`: 214 instancias de tipos de vidrio con 9 atributos numÃ©ricos y 6 clases

```
=== Cargar Dataset ===
1. Cargar dataset completo
2. Cargar conjuntos de entrenamiento y prueba
0. Volver al menÃº principal

Seleccione una opciÃ³n: 1
Nombre del archivo CSV: src/main/resources/iris.csv
Â¿El archivo tiene cabecera? (s/n): s
Delimitador (por defecto ','): 
Ãndices de atributos de clase (separados por comas): 4
```

> **Nota**: Para el dataset Iris, el Ã­ndice del atributo de clase es 4 (quinta columna).

### 2. Explorar InformaciÃ³n del Dataset

Puedes obtener estadÃ­sticas detalladas sobre los atributos:

```
=== InformaciÃ³n del Dataset Completo ===
NÃºmero de instancias: 150
NÃºmero de atributos: 5
Atributos de clase: iris
Tipo de preprocesamiento: NONE

=== Attributes Info ===
Atributo: sepal length (NumÃ©rico)
  MÃ­nimo: 4.3
  MÃ¡ximo: 7.9
  Media: 5.84
  DesviaciÃ³n tÃ­pica: 0.83
  Peso: 1.0
...
```

### 3. Preprocesar Datos

El preprocesamiento es esencial para obtener buenos resultados de clasificaciÃ³n:

```
=== Preprocesamiento ===
1. Sin preprocesamiento (datos crudos)
2. NormalizaciÃ³n rango [0,1]
3. EstandarizaciÃ³n (z-score)

Seleccione el tipo de preprocesamiento: 2
```

### 4. Configurar el Clasificador

Configure los parÃ¡metros del algoritmo k-NN:

```
=== ConfiguraciÃ³n del Clasificador KNN ===
Valor de k (nÃºmero de vecinos): 3

Seleccione la mÃ©trica de distancia:
1. Distancia EuclÃ­dea
2. Distancia de Manhattan
3. Distancia de Chebyshev
OpciÃ³n: 1

Seleccione la estrategia de peso de casos:
1. Igualdad de votos
2. CercanÃ­a (inversamente proporcional a la distancia)
3. Voto fijo segÃºn orden
OpciÃ³n: 2

Seleccione la regla de clasificaciÃ³n:
1. MayorÃ­a simple
2. Umbral (mayorÃ­a cualificada)
OpciÃ³n: 1
```

### 5. ExperimentaciÃ³n

EvalÃºe el rendimiento del clasificador:

```
=== ExperimentaciÃ³n ===
Porcentaje para conjunto de prueba (0-1): 0.3
Â¿Generar conjunto de prueba aleatoriamente? (s/n): s
Semilla para generaciÃ³n aleatoria (por defecto 1234): 

Ejecutando experimento...

=== Resultados del Experimento ===
PrecisiÃ³n Predictiva: 0.9556

Matriz de ConfusiÃ³n:
             Iris-setosa  Iris-versicolor  Iris-virginica  (Predicho)
Iris-setosa      15            0               0
Iris-versicolor   0            13              1
Iris-virginica    0            1               15
(Real)
```

### 6. Clasificar Nuevas Instancias

Predice la clase de nuevas observaciones:

```
=== Clasificar Nueva Instancia ===
Ingrese los valores para cada atributo (separados por comas):
0: sepal length
1: sepal width
2: petal length
3: petal width
4: iris (clase)
5.1,3.5,1.4,0.2,

Clase predicha: Iris-setosa
```

## ğŸ”¬ Ejemplos de ClasificaciÃ³n

### Dataset Iris
El dataset Iris es perfecto para ilustrar el funcionamiento del clasificador k-NN:

<table align="center">
<tr>
  <td align="center"><img src="https://imgur.com/a/TzeZIMu" width="400px"/></td>
</tr>
<tr>
  <td align="center"><b>Ejemplo de clasificaciÃ³n k-NN (k=3)</b></td>
</tr>
</table>

Con el preprocesamiento adecuado y k=3, el sistema puede lograr una precisiÃ³n predictiva superior al 95% en este dataset.

## ğŸ“˜ Fundamentos TeÃ³ricos

### El Algoritmo k-NN

El mÃ©todo de k-vecinos mÃ¡s cercanos (k-NN) es un algoritmo de aprendizaje supervisado basado en instancias. Su funcionamiento se basa en los siguientes principios:

1. **BÃºsqueda de vecinos**: Para clasificar una nueva instancia, se buscan las k instancias mÃ¡s cercanas del conjunto de entrenamiento.

2. **CÃ¡lculo de distancias**: La "cercanÃ­a" se determina mediante mÃ©tricas de distancia como:
    - **Distancia EuclÃ­dea**: $d(x,y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}$
    - **Distancia Manhattan**: $d(x,y) = \sum_{i=1}^{n} |x_i - y_i|$
    - **Distancia Chebyshev**: $d(x,y) = \max_{i=1..n} |x_i - y_i|$

3. **AsignaciÃ³n de clase**: La clase asignada a la nueva instancia depende de las clases de sus k vecinos mÃ¡s cercanos, considerando:
    - Estrategias de peso: Determina cuÃ¡nto "voto" tiene cada vecino
    - Reglas de clasificaciÃ³n: CÃ³mo se decide la clase final en base a los votos

4. **Preprocesamiento**: Normalizar o estandarizar los datos para evitar que atributos con valores altos dominen el cÃ¡lculo de distancias.

## ğŸ—ï¸ ImplementaciÃ³n y Arquitectura

El sistema estÃ¡ diseÃ±ado siguiendo principios de orientaciÃ³n a objetos y modularidad:

- **PatrÃ³n Strategy**: Usado para intercambiar diferentes mÃ©tricas, estrategias de peso y reglas de clasificaciÃ³n.
- **AbstracciÃ³n de atributos**: JerarquÃ­a de clases para manejar diferentes tipos de atributos.
- **SeparaciÃ³n de responsabilidades**: MÃ³dulos independientes para dataset, clasificaciÃ³n y experimentaciÃ³n.

## ğŸ“„ Licencia

Este proyecto estÃ¡ licenciado bajo la Licencia MIT - consulta el archivo LICENSE para mÃ¡s detalles.

---

Desarrollado por Daniel Alejandro Ãlvarez Casablanca - Grado en IngenierÃ­a InformÃ¡tica
